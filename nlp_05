word2vec（词向量）模型背后的基本思想是对出现在上下文环境里的词进行预测。对于每一条输入文本，我们选取一个上下文窗口和一个中心词，并基于这个中心词去预测窗口里其它词出现的概率。
因此word2vec模型可以方便地从新增语料中学习到新增词的向量表达，是一种高效的在线学习算法。

word2vec的主要思路：通过单词和上下文彼此预测，对应的两个算法分别为：
1：Skip-grams(SG):预测上下文
2：Continuous Bag of Words(CBOW)
Skip-grams(SG)是给定input word来预测上下文。而CBOW是给定上下文，来预测input word。

word2vec模型实际上分为了两个部分，第一部分为建立模型，第二部分是通过模型获取嵌入词向量。

霍夫曼树的原理：
1将权值为（w1,w2,......wn)的n个节点看做是有n颗树的森林，每个树仅仅有一个节点
2在森林中选择根节点权值最小的两颗树进行合并，得到一个新的树，这两棵树分别作为新树的左右子树。新树的跟节点权重为左右子树的跟节点权重只和
3将之前的根节点权值最小的两颗树丛森林删除，并把新树加入森林
4重复步骤2和3直到森林里只有一颗树为止。

TextCNN是利用CNN进行文本特征提取，不同大小的卷积核分别抽取n-gram特征，卷积计算出的特征图经过MaxPooling保留最大的特征值，然后将拼接成一个向量作为文本的表示。

TextRNN是利用RNN进行文本特征提取，由于本身是一种序列，而LSTM天然适合建模序列数据。TextRNN将句子中每个词的词向量依次输入到双向双层LSTM，
分别将两个方向最后一个有效位置的隐藏层拼接上一个向量作为文本的表示。
