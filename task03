主题：字符识别模型
主要包含2个部分：一是掌握CNN基本组成和原理，二是使用Pytorch框架构建CNN模型，并完成训练，主要笔记如下：
一：CNN全称卷积神经网络(Convolutional Neural Network),组成部分主要包含卷积层(涉及多个卷积核，填充，步长等几个主要因素)、
   池化层（最大池化层，最小池化层，均值池化层，使用最多的是最大池化层）、非线性激活函数（sigmod，Relu函数，tanx函数等）以及
   全连接层。
二：典型的卷积网络模型有：Le-Net5,AlexNet,VGG,Inceptionv3和ResNet50
三：在Pytorch中构建CNN模型非常简单，只需要定义好模型的参数和正向传播即可，Pytorch会根据正向传播自动计算方向传播。
四：附上代码如下
import os, sys, glob, shutil, json 
import cv2
from PIL import Image 
import numpy as np

import torch 
from torch.utils.data.dataset import Dataset 
import torchvision.transforms as transforms

class SVHNDataset(Dataset): 
    def __init__(self, img_path, img_label, transform=None):    
        self.img_path = img_path       
        self.img_label = img_label         
        if transform is not None:            
            self.transform = transform        
        else:            
            self.transform = None
                
    def __getitem__(self, index):    
        img = Image.open(self.img_path[index]).convert('RGB')
        if self.transform is not None:       
            img = self.transform(img)            
            lbl = np.array(self.img_label[index], dtype=np.int)
            lbl = list(lbl)  + (5 - len(lbl)) * [10]               
            return img, torch.from_numpy(np.array(lbl[:5]))
        
    def __len__(self):    
        return len(self.img_path)
train_path = glob.glob('./train/*.png') 
train_path.sort()
train_json = json.load(open('./train.json'))
train_label = [train_json[x]['label'] for x in train_json]
 
train_loader = torch.utils.data.DataLoader( 
       SVHNDataset(train_path, train_label,      
                   transforms.Compose([             
                   transforms.Resize((64, 128)),
                   transforms.ColorJitter(0.2, 0.2, 0.2),
                   transforms.RandomRotation(5),
                   transforms.ToTensor(),
                   transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])     
                   ])),
     batch_size=10, #每批样本个数
     shuffle=False,#是否打乱顺序
    #读取线程个数
     num_workers=0, 
)   

#for data in train_loader:  
#    print(data)

import torch 
torch.manual_seed(0)
torch.backends.cudnn.deterministic = False 
torch.backends.cudnn.benchmark = True

import torchvision.models as models
import torchvision.transforms as transforms
import torchvision.datasets as datasets 
import torch.nn as nn 
import torch.nn.functional as F 
import torch.optim as optim 
from torch.autograd import Variable 
from torch.utils.data.dataset import Dataset

#定义模型
class SVHN_Model1(nn.Module):  
    def __init__(self):        
        super(SVHN_Model1, self).__init__() 
        
        self.cnn = nn.Sequential(          
            nn.Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2)),    
            nn.ReLU(),           
            nn.MaxPool2d(2),          
            nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2)),   
            nn.ReLU(),           
            nn.MaxPool2d(2),      
        )       
        self.fc1 = nn.Linear(32*3*7, 11)      
        self.fc2 = nn.Linear(32*3*7, 11)    
        self.fc3 = nn.Linear(32*3*7, 11)     
        self.fc4 = nn.Linear(32*3*7, 11)      
        self.fc5 = nn.Linear(32*3*7, 11)       
        self.fc6 = nn.Linear(32*3*7, 11)     
        
    def forward(self, img):              
        feat = self.cnn(img)      
        feat = feat.view(feat.shape[0], -1)     
        c1 = self.fc1(feat)      
        c2 = self.fc2(feat)    
        c3 = self.fc3(feat)     
        c4 = self.fc4(feat)       
        c5 = self.fc5(feat)       
        c6 = self.fc6(feat)       
        return c1, c2, c3, c4, c5, c6   
    
    
model = SVHN_Model1()
#<div STYLE="page-break-after: always;"></div>

criterion = nn.CrossEntropyLoss() 
#<div STYLE="page-break-after: always;"></div> 
optimizer = torch.optim.Adam(model.parameters(), 0.005)

loss_plot, c0_plot = [], [] 

#<div STYLE="page-break-after: always;"></div> 

for epoch in range(10):   
    for data in train_loader:       
        c0, c1, c2, c3, c4, c5 = model(data[0])   
        loss = criterion(c0, data[1][:, 0]) + criterion(c1, data[1][:, 1]) + criterion(c2, data[1][:, 2]) + criterion(c3, data[1][:, 3]) + criterion(c4, data[1][:, 4]) + criterion(c5, data[1][:, 5])      
        loss /= 6 
        print(loss)
        optimizer.zero_grad()    
        loss.backward()   
        optimizer.step()           
        loss_plot.append(loss.item())     
        c0_plot.append((c0.argmax(1) == data[1][:, 0]).sum().item()*1.0 / c0.shape[0])     
        
   # print(epoch)
